#!/usr/bin/python
# 
# pg_summary 
#
#   Test the activity of a Postgres service during a small time frame.
# This tool is not a replacement for a monitoring/trending tool, instead is useful
# for first onboard reviews and checks.
#
# Pre-requisites: python-psycopg2

import psycopg2
import time
import sys
from optparse import OptionParser

#Constants and hardcoded/hard variables
VERSION = "Alpha? Not even!"
DSN = "dbname=postgres user=postgres"
DEF_PROC_TIME = 900 # That's 15 minutes
DEF_INTERVAL_TIME = 60 # That's 1 minute
DEF_DEBUG=2
DEBUG=2

def parse():
    parser = OptionParser(usage="usage: %prog -h [host] -p [port] -U [pguser]",
                          version="%prog " + VERSION )
    parser.add_option("-H", "--host",
                      action="store",
                      dest="host",
                      default="localhost",
                      help="Host to review")
    parser.add_option("-p", "--port",
                      action="store",
                      dest="port",
                      default=5432,
                      help="Postgres Port Default=5432")
    parser.add_option("-U", "--user",
                      action="store",
                      dest="pguser",
                      default="postgres",
                      help="Postgres user")
    parser.add_option("-d", "--database",
                      action="store",
                      dest="database",
                      default="postgres",
                      help="Database to connect")
    parser.add_option("-T", "--time-delay",
                      action="store",
                      dest="PROC_TIME",
                      default=DEF_PROC_TIME,
                      help="Time spent for the analysis")
    parser.add_option("-i", "--interval-time",
                      action="store",
                      dest="INTERVAL_TIME",
                      default=DEF_INTERVAL_TIME,
                      help="Interval between the inserts.")
    parser.add_option("-D","--debug",
                      action="store",
                      dest="DEBUG",
                      default=DEF_DEBUG,
                      help="Debug level")
    (options, args) = parser.parse_args()

#    if options.host == False:
#        parser.error("Error: host is required.")
    return (options, args)    

def debug(message,level=1):
    "Print a debug message"
    if DEBUG>=level:
        print time.asctime(),message
        # flush buffers
        sys.stdout.flush()

def create_schema(conn):
    conn.set_isolation_level(0)
    cur = conn.cursor()
    cur.execute("CREATE SCHEMA _summary_;")
    # droped id serial only for quick test due to insert conflicts
    cur.execute("CREATE TABLE _summary_.bgwriter (  ts timestamp default clock_timestamp(), "
                "checkpoints_timed bigint, checkpoints_req bigint, checkpoint_write_time bigint, checkpoint_sync_time bigint,"
                "buffers_checkpoint bigint, buffers_clean bigint, maxwritten_clean bigint, "
                "buffers_backend bigint, buffers_backend_fsync bigint, buffers_alloc bigint);")
    cur.execute("CREATE TABLE _summary_.databases (id serial PRIMARY KEY, ts timestamp default clock_timestamp(), "
                "datname name,numbackends  integer,xact_commit    bigint, xact_rollback   bigint,"    
                "blks_read    bigint, blks_hit    bigint, tup_returned   bigint, tup_fetched   bigint," 
                "tup_inserted  bigint,  tup_updated   bigint , tup_deleted   bigint , conflicts bigint,"
                "temp_files bigint , temp_bytes bigint ,deadlocks bigint ,blk_read_time   double precision, "
                "blk_write_time  double precision); ")
    

def drop_schema(conn):
    conn.set_isolation_level(0)
    cur = conn.cursor()
    cur.execute("DROP SCHEMA IF EXISTS _summary_ CASCADE")
#    return null


def collect_data(conn):
    cur = conn.cursor()
    cur.execute("INSERT INTO _summary_.bgwriter SELECT clock_timestamp(), checkpoints_timed , checkpoints_req , "
                "checkpoint_write_time , checkpoint_sync_time , buffers_checkpoint , buffers_clean , maxwritten_clean , "
                "buffers_backend , buffers_backend_fsync , buffers_alloc FROM pg_catalog.pg_stat_bgwriter;")
    #cur.execute("SELECT * from _summary_.bgwriter")
    #print cur.fetchone()


def main():
    (options, args) = parse()

    #connect to the database
    DSN = "dbname=" + options.database + " user=" + options.pguser + " port=" + str(options.port) + " host=" + options.host
    conn = psycopg2.connect(DSN)

    start = time.time()
    ETA =  time.time() + int(options.PROC_TIME)

    print start
    print ETA
    # Prepare the environment (tables) 
    try:
        create_schema(conn)
    except Exception, e:
        drop_schema(conn)
        debug("create_schema: %s" % str(e))
        #raise    
    # retrieve the data
    while ETA > time.time():
        collect_data(conn)    
        time.sleep(int(options.INTERVAL_TIME))

    cur = conn.cursor()
    cur.execute("SELECT * from _summary_.bgwriter")
    for all in cur.fetchall():
        print (all)
    # retrieve the new data
    #calculate the diff
    #report nicely
   
    #drop schema/tables
    drop_schema(conn)
    conn.close()


# call main
if __name__ == '__main__':
    main()

